{{- if .Values.logging.enabled -}}
# https://banzaicloud.com/docs/one-eye/logging-operator/install/#helm
apiVersion: plugins.kubegems.io/v1beta1
kind: Plugin
metadata:
  name: logging-operator
  namespace: "{{ .Release.Namespace }}"
spec:
  enabled: true
  kind: helm
  installNamespace: {{ .Values.logging.namespace }}
  repo: https://kubernetes-charts.banzaicloud.com
  version: {{ .Values.logging.version }}
  values:
    image:
      # repository: ghcr.io/banzaicloud/logging-operator
      {{ include "common.images.repository" ( dict "registry" "ghcr.io" "repository" "banzaicloud/logging-operator" "context" .) }}
    resources:
      requests:
        cpu: 100m
        memory: 128Mi
      limits:
        cpu: 500m
        memory: 512Mi
---
apiVersion: plugins.kubegems.io/v1beta1
kind: Plugin
metadata:
  name: logging-operator-logging
  namespace: "{{ .Release.Namespace }}"
spec:
  enabled: true
  kind: helm
  installNamespace: {{ .Values.logging.namespace }}
  repo: https://kubernetes-charts.banzaicloud.com
  version: {{ .Values.logging.version }}
  values:
    controlNamespace: {{ .Values.logging.namespace }}
    allowClusterResourcesFromAllNamespaces: false
    fluentbit:
      bufferStorage:
        storage.backlog.mem_limit: 5M 
        storage.path: /var/log/log-buffer
      extraVolumeMounts:
      - source: /data
        destination: /data
        readOnly: true
      bufferStorageVolume:
        hostPath:
          path: /var/log/log-buffer
      enableUpstream: false
      filterKubernetes:
        Use_Kubelet: "false"
      forwardOptions:
        Require_ack_response: false
      image:
        pullPolicy: IfNotPresent
        {{ include "common.images.repository" ( dict "repository" "kubegems/fluent-bit" "context" .) }}
        tag: 1.8.8-largebuf
      inputTail:
        Docker_Mode: "false"
        Mem_Buf_Limit: 256M
        Buffer_Chunk_Size: 256k
        Buffer_Max_Size: 256k
        {{- if eq .Values.global.runtime "docker" }}
        Parser: docker
        {{- else }}
        Parser: cri
        {{- end }}
        Rotate_Wait: "5"
        Skip_Long_Lines: "true"
      livenessDefaultCheck: true
      metrics:
        interval: 60s
        path: /api/v1/metrics/prometheus
        port: 2020
        prometheusRules: false
        {{- if .Values.monitoring.enabled }}
        serviceMonitor: true
        {{- else }}
        serviceMonitor: false
        {{- end }}
      network:
        connectTimeout: 30
        keepaliveIdleTimeout: 60
      positiondb:
        hostPath:
          path: /var/log/positiondb
      resources:
        limits:
          cpu: 1
          memory: 1Gi
        requests:
          cpu: 100m
          memory: 128Mi
      security:
        podSecurityPolicyCreate: true
        securityContext:
          allowPrivilegeEscalation: true
          capabilities:
            add:
            - SYS_RESOURCE
          privileged: true
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/master  
    fluentd:
      bufferStorageVolume:
        pvc:
          spec:
            accessModes:
            - ReadWriteOnce
            resources:
              requests:
                storage: 40Gi
            volumeMode: Filesystem
            storageClassName: {{ .Values.global.storageClass }}
      configReloaderImage:
        pullPolicy: IfNotPresent
        {{ include "common.images.repository" ( dict "repository" "kubegems/configmap-reload" "context" .) }}
        tag: v0.4.0
      fluentOutLogrotate:
        age: "7"
        enabled: true
        path: /fluentd/log/out
        size: "10485760"
      ignoreRepeatedLogInterval: 60s
      ignoreSameLogInterval: 60s
      image:
        pullPolicy: IfNotPresent
        {{ include "common.images.repository" ( dict "repository" "kubegems/fluentd" "context" .) }}
        tag: v1.13.3-alpine-11.1
      logLevel: info
      metrics:
        interval: 30s
        path: /metrics
        port: 24231
        prometheusRules: false
        {{- if .Values.monitoring.enabled }}
        serviceMonitor: true
        serviceMonitorConfig:
          honorLabels: true
        {{- else }}
        serviceMonitor: false
        {{- end }}
      resources:
        limits:
          cpu: 2
          memory: 4Gi
        requests:
          cpu: 100m
          memory: 256Mi
      scaling:
        replicas: 1
      security:
        podSecurityContext:
          fsGroup: 101
        podSecurityPolicyCreate: true
        roleBasedAccessControlCreate: true
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
      workers: 1     
    clusterOutputs: 
      - name: kubegems-container-console-output
        spec:
          loki:
            buffer:
              flush_interval: 1s
              flush_mode: interval
              flush_thread_count: 4
              tags: time
              timekey: 1m
              timekey_wait: 5s
              total_limit_size: 10G
            configure_kubernetes_labels: false
            drop_single_key: true
            extra_labels:
              cluster: {{ .Values.global.clusterName }}
            extract_kubernetes_labels: true
            labels:
              container: $.kubernetes.container_name
              image: $.kubernetes.container_image
              namespace: $.kubernetes.namespace_name
              node: $.kubernetes.host
              pod: $.kubernetes.pod_name
              stream: $.stream
            remove_keys:
            - logtag
            - kubernetes
            - docker
            - time
            - throttle_group_key
            url: {{ include "logging.loki.address" . }}
---
apiVersion: plugins.kubegems.io/v1beta1
kind: Plugin
metadata:
  name: loki-redis
  namespace: "{{ .Release.Namespace }}"
spec:
  name: redis
  enabled: true
  kind: helm
  installNamespace: {{ .Values.logging.namespace }}
  repo: https://charts.bitnami.com/bitnami
  version: 16.9.11
  values:
    architecture: standalone
    replica:
      replicaCount: 1
    auth:
      enabled: false
---
# https://github.com/grafana/helm-charts/tree/main/charts/loki
apiVersion: plugins.kubegems.io/v1beta1
kind: Plugin
metadata:
  name: loki
  namespace: "{{ .Release.Namespace }}"
spec:
  enabled: true
  kind: helm
  installNamespace: {{ .Values.logging.namespace }}
  repo: https://grafana.github.io/helm-charts
  version: {{ .Values.logging.values.loki.version }}
  values:
    image:
      # repository: docker.io/grafana/loki
      {{ include "common.images.repository" ( dict "repository" "grafana/loki" "context" .) }}
    resources:
      limits:
        cpu: 2
        memory: 4Gi
      requests:
        cpu: 500m
        memory: 1Gi
    serviceMonitor:
      enabled: true
    persistence:
      enabled: true
      accessModes:
      - ReadWriteOnce
      size: 10Gi
    config:
      auth_enabled: false
      server:
        http_listen_port: 3100
        grpc_server_max_recv_msg_size: 10000000
        grpc_server_max_send_msg_size: 10000000
        grpc_server_max_concurrent_streams: 0
      frontend:
        compress_responses: true
      query_range:
        results_cache:
          cache:
            redis:
              endpoint: loki-redis-headless:6379
              expiration: 1h
        cache_results: true
      compactor:
        shared_store: filesystem
        working_directory: /data/loki/boltdb-shipper-compactor
      ingester:
        lifecycler:
          address: 127.0.0.1
          ring:
            kvstore:
              store: inmemory
            replication_factor: 1
          final_sleep: 0s
        chunk_idle_period: 5m
        chunk_retain_period: 30
        wal:
          enabled: true
          dir: /data/wal
          flush_on_shutdown: true
      schema_config:
        configs:
        - from: "2022-02-01"
          index:
            period: 24h
            prefix: index_
          object_store: filesystem
          schema: v11
          store: boltdb-shipper
      storage_config:
        boltdb_shipper:
          active_index_directory: /data/loki/boltdb-shipper-active
          cache_location: /data/loki/boltdb-shipper-cache
          cache_ttl: 24h
          shared_store: filesystem
        filesystem:
          directory: /data/loki/chunks
        index_queries_cache_config:
          redis:
            endpoint: loki-redis-headless:6379
            expiration: 1h
      chunk_store_config:
        max_look_back_period: 0s
        chunk_cache_config:
          redis:
            endpoint: loki-redis-headless:6379
            expiration: 1h
        write_dedupe_cache_config:
          redis:
            endpoint: loki-redis-headless:6379
            expiration: 1h
      table_manager:
        chunk_tables_provisioning:
          inactive_read_throughput: 0
          inactive_write_throughput: 0
          provisioned_read_throughput: 0
          provisioned_write_throughput: 0
        index_tables_provisioning:
          inactive_read_throughput: 0
          inactive_write_throughput: 0
          provisioned_read_throughput: 0
          provisioned_write_throughput: 0
        retention_deletes_enabled: false
        retention_period: 0
      limits_config:
        split_queries_by_interval: 24h
        enforce_metric_name: false
        reject_old_samples: true
        reject_old_samples_max_age: 168h
        ingestion_rate_mb: 64
        ingestion_burst_size_mb: 128
        max_entries_limit_per_query: 50000
      ruler:
        storage:
          type: local
          local:
            directory: /rules
        rule_path: /tmp/rules
        alertmanager_url: {{ include "monitoring.alertmanager.address" .}}
        ring:
          kvstore:
            store: inmemory
        enable_api: true
    alerting_groups:
      - name: kubegems-loki-recoring-rules
        interval: 1m
        rules:
        - record: gems_loki_logs_count_last_1m
          expr: sum(count_over_time({stream=~"stdout|stderr"}[1m])) by (namespace, pod, container)
        - record: gems_loki_error_logs_count_last_1m
          expr: sum(count_over_time({stream=~"stdout|stderr"} |~ `error` [1m])) by (namespace, pod, container)
{{- end -}}